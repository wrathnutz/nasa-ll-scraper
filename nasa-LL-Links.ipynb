{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import os\n",
    "os.environ[\"LANG\"] = \"en_US.UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_url = 'https://llis.nasa.gov/lesson/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ff_options = Options()\n",
    "ff_options.add_argument(\"--headless\")\n",
    "\n",
    "driver = webdriver.Firefox(options=ff_options)\n",
    "\n",
    "def collect_lesson_links(base_url):\n",
    "    links = []\n",
    "    try:\n",
    "        driver.get(base_url)\n",
    "        # Wait for dynamic content to load    \n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a[href^=\"/lesson/\"]'))\n",
    "        )\n",
    "        # Collect all links that begin with \"/lesson/\"\n",
    "        links = [a.get_attribute('href') for a in driver.find_elements(By.CSS_SELECTOR, 'a[href^=\"/lesson/\"]')]\n",
    "        # Make links absolute\n",
    "        links = [f\"https://llis.nasa.gov{link}\" if link.startswith('/lesson/') else link for link in links]\n",
    "    except:\n",
    "        print(\"Error getting links from: \" + base_url)\n",
    "    return links\n",
    "\n",
    "def scrape_lesson_content(links):\n",
    "    data = []\n",
    "    for link in links:\n",
    "        driver.get(link)\n",
    "        # Allow some time for the page to load and JavaScript to render content\n",
    "        time.sleep(5)\n",
    "        # Use BeautifulSoup to parse page source and extract <p> text\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        for p in paragraphs:\n",
    "            # Ensure that paragraph text is not empty or repetitive\n",
    "            paragraph_text = p.text.strip()\n",
    "            if paragraph_text and paragraph_text not in [row[1] for row in data]:\n",
    "                data.append([link, paragraph_text])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list = []\n",
    "\n",
    "for i in range(1, 10000):\n",
    "    test_link = base_url + str(i)    \n",
    "    # try to collect some links\n",
    "    links = collect_lesson_links(test_link)\n",
    "    # See if anything was returned\n",
    "    if len(links) > 0:\n",
    "        # Lesson links were found, append the initial link        \n",
    "        # make sure the test link doesn't exist already\n",
    "        if test_link not in master_list:\n",
    "            master_list.append(test_link)\n",
    "        # Check that each link doesn't exist in master list\n",
    "        for link in links:\n",
    "            if link not in master_list:\n",
    "                master_list.append(link)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Found: \", len(master_list), \" links.\")\n",
    "master_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(master_list, columns=['URL'])\n",
    "df.to_csv('./Downloads/urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scrape content from each link\n",
    "#content = scrape_lesson_content(links)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "#df = pd.DataFrame(content, columns=['URL', 'Paragraph'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "#df.to_csv('./Downloads/lesson_content.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
